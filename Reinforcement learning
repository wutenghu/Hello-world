1. Basic Goal:
#  To discover an optimal policy *pi that maps states to actions so as to maximize the expected return J
>  myopic or greedy algorithm
>  two natural goals arises for the learner: find an optimal strategy at the end of a phase of training or interaction; and maximize the reward over the whole time the robot is interacting with the world
>  exploration-exploitation trade-off

2. RL in the average reward setting
>  mu_pi(s): the stationary state distribution generated by policy pi
>  pi(s,a) = P(a|s)

2.1  Value function approaches and Lagrange dual formulation to solve the problem defined through (1-3)
>  Dynamic programming-based methods (model-based)
>  Monte carlo methods (model-free)
>  Temporal difference methods

2.2  Policy search
>  Broadly break down policy-search methods into "black box" and "white box"

3. 



