Part A: 
1. Agent:
>  简单反射 Agent
>  基于模型的反射 Agent
>  基于目标的 Agent
>  基于效用的 Agent (Utility function)

2. 强化学习:
>  基于效用的 Agent 学习关于状态的效用函数
>  Q-学习 Agent 学习 行动-价值(Q 函数)
>  反射型 Agent 学习一种策略

2.1 被动强化学习
>  被动学习的任务类似于策略评价
>  自动效用估计； 自适应动态规划； 时序差分学习
2.2 主动强化学习


Part B: Background
1. Basic Goal:
#  To discover an optimal policy *pi that maps states to actions so as to maximize the expected return J
>  myopic or greedy algorithm
>  two natural goals arises for the learner: find an optimal strategy at the end of a phase of training or interaction; and maximize the reward over the whole time the robot is interacting with the world
>  exploration-exploitation trade-off


2. RL in the average reward setting
>  mu_pi(s): the stationary state distribution generated by policy pi
>  pi(s,a) = P(a|s)

2.1  Value function approaches and Lagrange dual formulation to solve the problem defined through (1-3)
>  Dynamic programming-based methods (model-based)
>  Monte carlo methods (model-free)
>  Temporal difference methods

2.2  Policy Search
>  Broadly break down policy-search methods into "black box" and "white box"

3. Typical problems
3.1  MDP vs POMDP
>  A markov decision process is just like a Markov Chain
>  Solving algorithms: Value Iteration Algorithm & Policy Iteration Algorithm


Part C: Literature and Resource Review
1. POMDP page: http://cs.brown.edu/research/ai/pomdp/index.html

2. Google Deepmind: https://deepmind.com/blog/deep-reinforcement-learning/
>  Natural letter: Human level control through deep reinforcement learning
>  Atari 2600 emulator: https://stella-emu.github.io/

3. Simple Reinforcement Learning with Tensorflow:
https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0

4. Book <Artificial Intelligence: A Modern Approach>: http://aima.cs.berkeley.edu/


