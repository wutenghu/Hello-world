Part A: Neural network hyper-parameters
>> Mini-Batch Gradient Descent Hyper-parameters
1. Learning-rate
> initial learning rate: epsilon_0, which is usually set ranges from 1 to 1e-6;

2. Loss-function
> Squared euclidian distance & cross-entropy

3. Mini-batch and stochastic optimizer (Colin Raffel)
> First, since the dataset used is often too large to fit in memory and/or be optimized
> efficiently; Second, the objective function is typically non-convex, so using different
> data at each iteration can avoid the optimization stack into a local minimum

> About the batch-size, due to it should not effect generalization/test performance, 
> thus the size (B) can be set first and then fixed while the other hyper-parameters 
> are optimized.

> Stochastic Gradient Descent up-grade the model at each data feeding iteration:
> theta_(n+1) = theta_n - eta*gradient_n

> In the SGD, the gradient usually changes rapidly, one very common technique is to "smooth"
> the gradient updates using a leaky integrator filter with parameter beta by: 
> g_(n+1) <- miu*g_n + eta*gradient_n

> In the Nesterov's Accelerated Gradient (NAG), the "acceleration" is fullfiled through 
> applying the predicted theata to approximate the gradient, and using the approximated
> gradient to update the theata instead of the previous gradient

> Adam:

4. Number of training iterations


6. Number of Hidden Units


7. Weight Decay
> A regularization is required to reduce the risk of training over-fitting, one normal
> requirement is to encourage the sum(theata) to be small (close to zero). L1 and L2
> regularizations (lagrange multipliers)

8. Activation Sparsity


9. Non-linearity
> Commonly used nonliearities: sigmoid; tanh; rectifier [ReLU = max(0,x)]; hard tanh; 
> **Rectifier and step units don't make sense for the output, because of they are not
> able to pass back error gradient into network

10. Weight Initialization


11. Random Seeds and Model Averaging


12. Preprocessing Input Data
> Element-wise standarization, principal component analysis, uniformization,
> non-linearities (logarithm or square root)

13.

Part B: Hyperparameter Sapce Exploration
1. Coordinate Descent
> Keep all parameters fixed and vary one parameter till the minimum of validation error!

2. Grid Search
> Nesting the parameter space, the computational expense is exponential in the number of
> parameters. can be easily parallelized

3. Random Search
> Avoid skipping the optimum of the hyper-sapce due to too large space step, easy to be
> parallelized

4. Model-based Search
> The sequential model-based optimization (SMBO) method is often employed to seek the optimum
> of functions that are non-derivative. According to the way of predicting the p(configuration|theta),
> the SMBO method can be further catergorized into: sequential model-based optimization of algorithm 
> configuration (SMAC), Tree-structured Parzen Estimation (TPE), Random Online Aggressive Racing (ROAR),
> Spearmint. In the review of Salvador et al. (2016), SMAC overperforms ROAR, Spearmint does not support 
> conditional attributes.

> open libraries:
> -- SMAC tool, which includes both SMAC and ROAR methods
> -- Hyperopt, which is a python library including random search and TPE
> -- HPOLib, which is a wrapper for SMAC, TPE and Spearmint methods
> -- Auto-Sklearn, which uses HPOLib and meta-learning to auto-select scikit-learn methods
> -- Auto-WEKA, which uses either SMAC or TPE to automatically select and optimise 39 classifiers 
>    and 24 possible feature selection methods in the WEKA

5. Reference:
[1] Practical recommendations for gradient-based training of deep architectures, 
> Yoshua Bengio, U. Montreal, arXiv report:1206.5533, Lecture Notes in Computer 
> Science Volume 7700, Neural Networks: Tricks of the Trade Second Edition, Editors: 
> Grégoire Montavon, Geneviève B. Orr, Klaus-Robert Müller, 2012.
[2] http://jaberg.github.io/hyperopt/
[3] https://github.com/JasperSnoek/spearmint
[4] http://www.cs.ubc.ca/labs/beta/Projects/SMAC/
[5] Colin Raffel, Stochastic optimization techniques, http://colinraffel.com/wiki/stochastic_optimization_techniques
[6] F. Hutter, H. Hoos, and K. Leyton-Brown, Bayesian Optimization with censored response data, NIPS 2011
[7] F. Hutter, H. Hoos, and K. Leyton-Brown, Sequential model-based optimization for general algorithm configuration, LION 2011
*** 
[8] F. Hutter, H. Hoos, and K. Leyton-Brown, An evaluation of sequential model-based optimization for expensive blackbox functions, LION 2013
*** sequential parameter optimziation (SPO) toolbox, SMAC's model are based on random forests 
    in instead of the gaussian process due to the random forest method outperform over the gaussian
    process method on the catergorical problems
[9] M.M. Salvador, M. Budka, and B. Gabrys, Towards automatic composition of multicomponent predictive systems, HAIS 2016
[10] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kegl, Algorithms for hyper-parameter optimization, NIPS 2011
[11] J. Snoek, H. Larochelle, and R.P. Adams, Practical bayesian optimization of machine learning algorithms, NIPS 2012
[12] K. Eggensperger, M. Feurer, and F. Hutter, Towards an empirical foundation for assessing bayesian optimization of hyper-parameters, NIPS 2013
[13] M. Feurer, A. Klein, K. Eggensperger, J.T. Springenberg, M. Blum, and F. Hutter, Methods for improving bayesian optimization for autoML, ICML 2015
[14] C. Thornton, F. Hutter, H.H. Hoos, and K. Leyton-Brown, Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms, ACM 2013
[15] I. Loshchilov, and F. Hutter, CMA-ES for hyperparameter optimization of deep neural networks, ICLR 2016
[16] D.R. Jones, M. Schonlau, and W.J. Welch, Efficient global optimization of expensive black box functions. JGO 1998

