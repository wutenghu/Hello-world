Part A: Neural network hyper-parameters
>> Mini-Batch Gradient Descent Hyper-parameters
1. Learning-rate
> initial learning rate: epsilon_0, which is usually set ranges from 1 to 1e-6;

2. Loss-function
> Squared euclidian distance & cross-entropy

3. Mini-batch and stochastic optimizer (Colin Raffel)
> First, since the dataset used is often too large to fit in memory and/or be optimized
> efficiently; Second, the objective function is typically non-convex, so using different
> data at each iteration can avoid the optimization stack into a local minimum

> About the batch-size, due to it should not effect generalization/test performance, 
> thus the size (B) can be set first and then fixed while the other hyper-parameters 
> are optimized.

> Stochastic Gradient Descent up-grade the model at each data feeding iteration:
> theta_(n+1) = theta_n - eta*gradient_n

> In the SGD, the gradient usually changes rapidly, one very common technique is to "smooth"
> the gradient updates using a leaky integrator filter with parameter beta by: 
> g_(n+1) <- miu*g_n + eta*gradient_n

> In the Nesterov's Accelerated Gradient (NAG), the "acceleration" is fullfiled through 
> applying the predicted theata to approximate the gradient, and using the approximated
> gradient to update the theata instead of the previous gradient

> Adam:

4. Number of training iterations


6. Number of Hidden Units


7. Weight Decay
> A regularization is required to reduce the risk of training over-fitting, one normal
> requirement is to encourage the sum(theata) to be small (close to zero). L1 and L2
> regularizations (lagrange multipliers)

8. Activation Sparsity


9. Non-linearity
> Commonly used nonliearities: sigmoid; tanh; rectifier [ReLU = max(0,x)]; hard tanh; 
> **Rectifier and step units don't make sense for the output, because of they are not
> able to pass back error gradient into network

10. Weight Initialization


11. Random Seeds and Model Averaging


12. Preprocessing Input Data
> Element-wise standarization, principal component analysis, uniformization,
> non-linearities (logarithm or square root)

13.

Part B: Hyperparameter Sapce Exploration
1. Coordinate Descent
> Keep all parameters fixed and vary one parameter till the minimum of validation error!

2. Grid Search
> Nesting the parameter space, the computational expense is exponential in the number of
> parameters. can be easily parallelized

3. Random Search
> Avoid skipping the optimum of the hyper-sapce due to too large space step, easy to be
> parallelized

4. Model-based serach
> Two major approaches have been widely employed: Bayesian optimization based on the Gaussian
> random process and on the tree-structured Parzen estimater. Available resources include the
> Hyperopt, Spearmint and SMAC packages.

5. Reference:
[1] Practical recommendations for gradient-based training of deep architectures, 
> Yoshua Bengio, U. Montreal, arXiv report:1206.5533, Lecture Notes in Computer 
> Science Volume 7700, Neural Networks: Tricks of the Trade Second Edition, Editors: 
> Grégoire Montavon, Geneviève B. Orr, Klaus-Robert Müller, 2012.
[2] http://jaberg.github.io/hyperopt/
[3] https://github.com/JasperSnoek/spearmint
[4] http://www.cs.ubc.ca/labs/beta/Projects/SMAC/
[5] Colin Raffel, Stochastic optimization techniques, http://colinraffel.com/wiki/stochastic_optimization_techniques
