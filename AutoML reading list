1. Definition
> automatic feature engineering/(selection, transformation, extraction) + automatic model training + automatic model deployment

2. Literature review
2.1 Automatic feature engineering
[1] Guyon I, Elisseeff A. An introduction to variable and feature selection[J]. Journal of machine learning research, 2003, 3(Mar): 1157-1182.
[2] Kanter J M, Veeramachaneni K. Deep feature synthesis: Towards automating data science endeavors[C]//Data Science and Advanced Analytics (DSAA), 2015. 36678 2015. IEEE International Conference on. IEEE, 2015: 1-10. http://www.jmaxkanter.com/static/papers/DSAA_DSM_2015.pdf
https://dspace.mit.edu/bitstream/handle/1721.1/107031/971493308-MIT.pdf?sequence=1
[3] Molina, Luis Carlos, Lluís Belanche, and Àngela Nebot. "Feature selection algorithms: A survey and experimental evaluation." Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002.
[4] Vanaja, S., and K. Ramesh Kumar. "Analysis of feature selection algorithms on classification: a survey." International Journal of Computer Applications 96.17 (2014).
[5] Cheng, Weiwei, et al. "Automated feature generation from structured knowledge." Proceedings of the 20th ACM international conference on Information and knowledge management. ACM, 2011.

2.2 Model selection/combination
[1] E. Brochu, V. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions,with application to active user modeling and hierarchical reinforcement learning.CoRR, abs/1012.2599,2010.  https://arxiv.org/pdf/1012.2599.pdf
[2] F. Hutter, H. Hoos, and K. Leyton-Brown.  Sequential model-based optimization for general algorithm configuration. InProc. of LION’11, pages 507–523, 2011.
[3] J. Bergstra, R. Bardenet, Y. Bengio, and B. K´egl. Algorithms for hyper-parameter optimization. InProc.of NIPS’11, pages 2546–2554, 2011.
[4] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms.InProc. of NIPS’12, pages 2960–2968, 2012.
[5] K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown.  Towardsan empirical foundation for assessing Bayesian optimization of hyperparameters. InNIPS Workshop onBayesian Optimization in Theory and Practice, 2013.
[6] P. Brazdil, C. Giraud-Carrier, C. Soares, and R. Vilalta.Metalearning:  Applications to Data Mining.Springer, 2009.
[7] R. Bardenet, M. Brendel, B. K´egl, and M. Sebag.  Collaborative hyperparameter tuning.  InProc. ofICML’13[28], pages 199–207.
[8] D. Yogatama and G. Mann. Efficient transfer learning method for automatic hyperparameter tuning. InProc. of AISTATS’14, pages 1077–1085, 2014
[9] A. Kalousis.Algorithm Selection via Meta-Learning. PhD thesis, University of Geneve, 2002.
[10] A. Lacoste, M. Marchand, F. Laviolette, and H. Larochelle. Agnostic Bayesian learning of ensembles. InProc. of ICML’14, pages 611–619, 2014.
[11] Y. Bengio. Gradient-based optimization of hyperparam-eters.Neural Computation, 12(8):1889–1900, 2000.
[12] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl. Algo-rithms for Hyper-Parameter Optimization. InProc. ofNIPS-11, 2011
[13] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization.JMLR, 13:281–305, 2012.
[14] A. Frank and A. Asuncion. UCI machine learning repos-itory, 2010. URL: http://archive.ics.uci.edu/ml. Uni-versity of California, Irvine, School of Information andComputer Sciences.
[15] Hydra:Automatically configuring algorithms for portfolio-basedselection
[16] Zoph B, Le Q V. Neural architecture search with reinforcement learning[J]. arXiv preprint arXiv:1611.01578, 2016. https://openreview.net/pdf?id=r1Ue8Hcxg
[17] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,and Nando de Freitas.  Learning to learn by gradient descent by gradient descent.arXiv preprintarXiv:1606.04474, 2016.
[18] Ke Li and Jitendra Malik. Learning to optimize.arXiv preprint arXiv:1606.01885, 2016

2.3 Systems
[1] Feurer, Matthias, et al. "Efficient and robust automated machine learning." Advances in Neural Information Processing Systems. 2015.  https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf
[2] C. Thornton, F. Hutter, H. Hoos, and K. Leyton-Brown. Auto-WEKA: combined selection and hyperpa-rameter optimization of classification algorithms. InProc. of KDD’13, pages 847–855, 2013. https://arxiv.org/pdf/1208.3719.pdf


2.4 Datasets and challenges
[1] I. Guyon, K. Bennett, G. Cawley, H. Escalante, S. Escalera, T. Ho, N.Maci`a, B. Ray, M. Saeed, A. Statnikov,and E. Viegas. Design of the 2015 ChaLearn AutoML Challenge. InProc. of IJCNN’15, 2015
[2] J. Vanschoren, J. van Rijn, B. Bischl, and L. Torgo.  OpenML: Networked science in machine learning.SIGKDD Explorations, 15(2):49–60, 2013.
